{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pythia_captioning_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucsky/dotfiles/blob/master/pythia_captioning_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXeEePmeEuuY",
        "colab_type": "text"
      },
      "source": [
        "## First download all of the necessary data\n",
        "\n",
        "---\n",
        "\n",
        "Press \"Shift + Enter\" to run each cell sequentially. Alternatively, you can press \"Cmd/Ctrl + F9\" to run all cells and then scroll down to bottom cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRUp0r_-B9N0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Pre-requisites needed for running the e2e model\n",
        "%cd /content/\n",
        "\n",
        "%mkdir model_data\n",
        "!wget -O /content/model_data/vocabulary_captioning_thresh5.txt https://dl.fbaipublicfiles.com/pythia/data/vocabulary_captioning_thresh5.txt\n",
        "!wget -O /content/model_data/detectron_model.pth  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.pth \n",
        "!wget -O /content/model_data/butd.pth https://dl.fbaipublicfiles.com/pythia/pretrained_models/coco_captions/butd.pth\n",
        "!wget -O /content/model_data/butd.yaml https://dl.fbaipublicfiles.com/pythia/pretrained_models/coco_captions/butd.yml\n",
        "!wget -O /content/model_data/detectron_model.yaml https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.yaml\n",
        "!wget -O /content/model_data/detectron_weights.tar.gz https://dl.fbaipublicfiles.com/pythia/data/detectron_weights.tar.gz\n",
        "!tar xf /content/model_data/detectron_weights.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4S01cUqE3WJ",
        "colab_type": "text"
      },
      "source": [
        "## Now, install some particular dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQCyXjYyFQzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install dependencies\n",
        "!pip install ninja yacs cython matplotlib demjson\n",
        "!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNW1WWWyFkB5",
        "colab_type": "text"
      },
      "source": [
        "## Install fastText for installing Pythia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke7OQtzeFV7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/\n",
        "%rm -rf fastText\n",
        "!git clone https://github.com/facebookresearch/fastText.git fastText\n",
        "%cd /content/fastText\n",
        "!pip install -e ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivSdn9BFFpxp",
        "colab_type": "text"
      },
      "source": [
        "## Install Pythia now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOHchoDW7yqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/\n",
        "%rm -rf pythia\n",
        "!git clone https://github.com/facebookresearch/pythia.git pythia\n",
        "%cd /content/pythia\n",
        "# Don't modify torch version\n",
        "!sed -i '/torch/d' requirements.txt\n",
        "!pip install -e .\n",
        "import sys\n",
        "sys.path.append('/content/pythia')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5o-zqvuFxeR",
        "colab_type": "text"
      },
      "source": [
        "## Install maskrcnn-benchmark now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1ROyH7yG11V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install maskrcnn-benchmark to extract detectron features\n",
        "%cd /content\n",
        "!git clone https://gitlab.com/meetshah1995/vqa-maskrcnn-benchmark.git\n",
        "%cd /content/vqa-maskrcnn-benchmark\n",
        "# Compile custom layers and build mask-rcnn backbone\n",
        "!python setup.py build\n",
        "!python setup.py develop\n",
        "sys.path.append('/content/vqa-maskrcnn-benchmark')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yx6FeDEF2sw",
        "colab_type": "text"
      },
      "source": [
        "## Demo\n",
        "\n",
        "The class handles everything from feature extraction, token extraction and predicting the answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCD0nso8YelA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import yaml\n",
        "import cv2\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "import gc\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from ipywidgets import widgets, Layout\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "from maskrcnn_benchmark.config import cfg\n",
        "from maskrcnn_benchmark.layers import nms\n",
        "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
        "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
        "from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n",
        "\n",
        "\n",
        "from pythia.utils.configuration import ConfigNode\n",
        "from pythia.tasks.processors import VocabProcessor, CaptionProcessor\n",
        "from pythia.models.butd import BUTD\n",
        "from pythia.common.registry import registry\n",
        "from pythia.common.sample import Sample, SampleList\n",
        "\n",
        "class PythiaDemo:\n",
        "  TARGET_IMAGE_SIZE = [448, 448]\n",
        "  CHANNEL_MEAN = [0.485, 0.456, 0.406]\n",
        "  CHANNEL_STD = [0.229, 0.224, 0.225]\n",
        "  \n",
        "  def __init__(self):\n",
        "    self._init_processors()\n",
        "    self.pythia_model = self._build_pythia_model()\n",
        "    self.detection_model = self._build_detection_model()\n",
        "    \n",
        "  def _init_processors(self):\n",
        "    with open(\"/content/model_data/butd.yaml\") as f:\n",
        "      config = yaml.load(f)\n",
        "    \n",
        "    config = ConfigNode(config)\n",
        "    # Remove warning\n",
        "    config.training_parameters.evalai_inference = True\n",
        "    registry.register(\"config\", config)\n",
        "    \n",
        "    self.config = config\n",
        "    \n",
        "    captioning_config = config.task_attributes.captioning.dataset_attributes.coco\n",
        "    text_processor_config = captioning_config.processors.text_processor\n",
        "    caption_processor_config = captioning_config.processors.caption_processor\n",
        "    \n",
        "    text_processor_config.params.vocab.vocab_file = \"/content/model_data/vocabulary_captioning_thresh5.txt\"\n",
        "    caption_processor_config.params.vocab.vocab_file = \"/content/model_data/vocabulary_captioning_thresh5.txt\"\n",
        "    self.text_processor = VocabProcessor(text_processor_config.params)\n",
        "    self.caption_processor = CaptionProcessor(caption_processor_config.params)\n",
        "\n",
        "    registry.register(\"coco_text_processor\", self.text_processor)\n",
        "    registry.register(\"coco_caption_processor\", self.caption_processor)\n",
        "    \n",
        "  def _build_pythia_model(self):\n",
        "    state_dict = torch.load('/content/model_data/butd.pth')\n",
        "    model_config = self.config.model_attributes.butd\n",
        "    model_config.model_data_dir = \"/content/\"\n",
        "    model = BUTD(model_config)\n",
        "    model.build()\n",
        "    model.init_losses_and_metrics()\n",
        "    \n",
        "    if list(state_dict.keys())[0].startswith('module') and \\\n",
        "       not hasattr(model, 'module'):\n",
        "      state_dict = self._multi_gpu_state_to_single(state_dict)\n",
        "          \n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(\"cuda\")\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  def _multi_gpu_state_to_single(self, state_dict):\n",
        "    new_sd = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if not k.startswith('module.'):\n",
        "            raise TypeError(\"Not a multiple GPU state of dict\")\n",
        "        k1 = k[7:]\n",
        "        new_sd[k1] = v\n",
        "    return new_sd\n",
        "  \n",
        "  def predict(self, url):\n",
        "    with torch.no_grad():\n",
        "      detectron_features = self.get_detectron_features(url)\n",
        "\n",
        "      sample = Sample()\n",
        "      sample.dataset_name = \"coco\"\n",
        "      sample.dataset_type = \"test\"\n",
        "      sample.image_feature_0 = detectron_features\n",
        "      sample.answers = torch.zeros((5, 10), dtype=torch.long)\n",
        "\n",
        "      sample_list = SampleList([sample])\n",
        "      sample_list = sample_list.to(\"cuda\")\n",
        "\n",
        "      tokens = self.pythia_model(sample_list)[\"captions\"]\n",
        "    \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return tokens\n",
        "    \n",
        "  \n",
        "  def _build_detection_model(self):\n",
        "\n",
        "      cfg.merge_from_file('/content/model_data/detectron_model.yaml')\n",
        "      cfg.freeze()\n",
        "\n",
        "      model = build_detection_model(cfg)\n",
        "      checkpoint = torch.load('/content/model_data/detectron_model.pth', \n",
        "                              map_location=torch.device(\"cpu\"))\n",
        "\n",
        "      load_state_dict(model, checkpoint.pop(\"model\"))\n",
        "\n",
        "      model.to(\"cuda\")\n",
        "      model.eval()\n",
        "      return model\n",
        "  \n",
        "  def get_actual_image(self, image_path):\n",
        "      if image_path.startswith('http'):\n",
        "          path = requests.get(image_path, stream=True).raw\n",
        "      else:\n",
        "          path = image_path\n",
        "      \n",
        "      return path\n",
        "\n",
        "  def _image_transform(self, image_path):\n",
        "      path = self.get_actual_image(image_path)\n",
        "\n",
        "      img = Image.open(path)\n",
        "      im = np.array(img).astype(np.float32)\n",
        "      im = im[:, :, ::-1]\n",
        "      im -= np.array([102.9801, 115.9465, 122.7717])\n",
        "      im_shape = im.shape\n",
        "      im_size_min = np.min(im_shape[0:2])\n",
        "      im_size_max = np.max(im_shape[0:2])\n",
        "      im_scale = float(800) / float(im_size_min)\n",
        "      # Prevent the biggest axis from being more than max_size\n",
        "      if np.round(im_scale * im_size_max) > 1333:\n",
        "           im_scale = float(1333) / float(im_size_max)\n",
        "      im = cv2.resize(\n",
        "           im,\n",
        "           None,\n",
        "           None,\n",
        "           fx=im_scale,\n",
        "           fy=im_scale,\n",
        "           interpolation=cv2.INTER_LINEAR\n",
        "       )\n",
        "      img = torch.from_numpy(im).permute(2, 0, 1)\n",
        "      return img, im_scale\n",
        "\n",
        "\n",
        "  def _process_feature_extraction(self, output,\n",
        "                                 im_scales,\n",
        "                                 feat_name='fc6',\n",
        "                                 conf_thresh=0.2):\n",
        "      batch_size = len(output[0][\"proposals\"])\n",
        "      n_boxes_per_image = [len(_) for _ in output[0][\"proposals\"]]\n",
        "      score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
        "      score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
        "      feats = output[0][feat_name].split(n_boxes_per_image)\n",
        "      cur_device = score_list[0].device\n",
        "\n",
        "      feat_list = []\n",
        "\n",
        "      for i in range(batch_size):\n",
        "          dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
        "          scores = score_list[i]\n",
        "\n",
        "          max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
        "\n",
        "          for cls_ind in range(1, scores.shape[1]):\n",
        "              cls_scores = scores[:, cls_ind]\n",
        "              keep = nms(dets, cls_scores, 0.5)\n",
        "              max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n",
        "                                           cls_scores[keep],\n",
        "                                           max_conf[keep])\n",
        "\n",
        "          keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n",
        "          feat_list.append(feats[i][keep_boxes])\n",
        "      return feat_list\n",
        "\n",
        "  def masked_unk_softmax(self, x, dim, mask_idx):\n",
        "      x1 = F.softmax(x, dim=dim)\n",
        "      x1[:, mask_idx] = 0\n",
        "      x1_sum = torch.sum(x1, dim=1, keepdim=True)\n",
        "      y = x1 / x1_sum\n",
        "      return y\n",
        "    \n",
        "  def get_detectron_features(self, image_path):\n",
        "      im, im_scale = self._image_transform(image_path)\n",
        "      img_tensor, im_scales = [im], [im_scale]\n",
        "      current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
        "      current_img_list = current_img_list.to('cuda')\n",
        "      with torch.no_grad():\n",
        "          output = self.detection_model(current_img_list)\n",
        "      feat_list = self._process_feature_extraction(output, im_scales, \n",
        "                                                  'fc6', 0.2)\n",
        "      return feat_list[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3IanIVPt91G",
        "colab_type": "text"
      },
      "source": [
        "### If the command below fails with 'CUDNN_EXECUTION_FAILED', try rerunning the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwF36OmQ72ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "demo = PythiaDemo()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CinwDLv5GLJI",
        "colab_type": "text"
      },
      "source": [
        "## Use the text fields below to ask a question on an image\n",
        "\n",
        "Image URL can be any http/https URL. We show top 5 predictions from Pythia. Confidence shows how confident Pythia model was about a particular prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_55MoLFlhL4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_widgets(url):\n",
        "  image_text = widgets.Text(\n",
        "    description=\"Image URL\", layout=Layout(minwidth=\"70%\")\n",
        "  )\n",
        "\n",
        "  image_text.value = url\n",
        "  submit_button = widgets.Button(description=\"Caption the image!\")\n",
        "\n",
        "  display(image_text)\n",
        "  display(submit_button)\n",
        "\n",
        "  submit_button.on_click(lambda b: on_button_click(\n",
        "      b, image_text\n",
        "  ))\n",
        "  \n",
        "  return image_text\n",
        "  \n",
        "def on_button_click(b, image_text):\n",
        "  clear_output()\n",
        "  image_path = demo.get_actual_image(image_text.value)\n",
        "  image = Image.open(image_path)\n",
        "  \n",
        "  tokens = demo.predict(image_text.value)\n",
        "  answer = demo.caption_processor(tokens.tolist()[0])[\"caption\"]\n",
        "  init_widgets(image_text.value)\n",
        "  display(image)\n",
        "  \n",
        "  display(HTML(answer))\n",
        " \n",
        "\n",
        "image_text = init_widgets(\n",
        "    \"http://images.cocodataset.org/train2017/000000505539.jpg\"\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}